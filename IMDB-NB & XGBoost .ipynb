{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-12T20:09:27.691434Z","iopub.execute_input":"2022-03-12T20:09:27.691908Z","iopub.status.idle":"2022-03-12T20:09:27.72994Z","shell.execute_reply.started":"2022-03-12T20:09:27.691816Z","shell.execute_reply":"2022-03-12T20:09:27.728999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n###\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import BernoulliNB\n####\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n#####\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# Creation of confusion matrix in using sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import plot_confusion_matrix\n","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:27.731673Z","iopub.execute_input":"2022-03-12T20:09:27.731923Z","iopub.status.idle":"2022-03-12T20:09:28.651865Z","shell.execute_reply.started":"2022-03-12T20:09:27.731889Z","shell.execute_reply":"2022-03-12T20:09:28.651179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Extraction and Preprocessing**","metadata":{}},{"cell_type":"code","source":"df_reviews = pd.read_json('../input/imdb-spoiler-dataset/IMDB_reviews.json', lines=True)\ndf_details = pd.read_json('../input/imdb-spoiler-dataset/IMDB_movie_details.json',lines =True)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:28.655059Z","iopub.execute_input":"2022-03-12T20:09:28.655263Z","iopub.status.idle":"2022-03-12T20:09:52.027956Z","shell.execute_reply.started":"2022-03-12T20:09:28.655237Z","shell.execute_reply":"2022-03-12T20:09:52.027228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_reviews.shape)\nprint(df_details.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:52.029905Z","iopub.execute_input":"2022-03-12T20:09:52.030261Z","iopub.status.idle":"2022-03-12T20:09:52.036973Z","shell.execute_reply.started":"2022-03-12T20:09:52.030229Z","shell.execute_reply":"2022-03-12T20:09:52.036152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reviews.head()                                                                                                                                                                                                                                                                                                                            ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:52.038595Z","iopub.execute_input":"2022-03-12T20:09:52.039349Z","iopub.status.idle":"2022-03-12T20:09:52.05957Z","shell.execute_reply.started":"2022-03-12T20:09:52.03931Z","shell.execute_reply":"2022-03-12T20:09:52.058947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_details.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:52.061131Z","iopub.execute_input":"2022-03-12T20:09:52.061369Z","iopub.status.idle":"2022-03-12T20:09:52.075265Z","shell.execute_reply.started":"2022-03-12T20:09:52.061336Z","shell.execute_reply":"2022-03-12T20:09:52.074298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reviews[\"is_spoiler\"].value_counts().plot(kind= \"bar\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:52.076753Z","iopub.execute_input":"2022-03-12T20:09:52.07775Z","iopub.status.idle":"2022-03-12T20:09:52.275617Z","shell.execute_reply.started":"2022-03-12T20:09:52.077657Z","shell.execute_reply":"2022-03-12T20:09:52.274938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Extract the information from the data\nspoiler_df = pd.DataFrame()\nspoiler_df[\"is_spoiler\"] = df_reviews[\"is_spoiler\"] \nspoiler_df[\"has_a_word_spoiler\"] = df_reviews[\"review_text\"].apply(lambda \n                                                                  text : True if \"spoiler\" in text \n                                                                  else False)\n\npie1 = spoiler_df['is_spoiler'].value_counts().reset_index().sort_values(by='index')\npie2 = spoiler_df[\"has_a_word_spoiler\"].value_counts().reset_index().sort_values(by='index')\n\nwith plt.style.context('seaborn-talk'):\n    fig = plt.figure(figsize=(16, 8))\n\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2)\n\n    ax1.pie(pie1['is_spoiler'])\n    ax1.set_title('All reviews')\n\n    ax2.pie(pie2['has_a_word_spoiler'])\n    ax2.set_title('Reviews containing the word \\'spoiler\\'')\n\n    plt.suptitle('Spoiler distribution within the reviews', fontsize=20)\n    fig.legend(labels=['Without spoilers(False)', 'With spoilers(True)'], loc='center')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:52.276846Z","iopub.execute_input":"2022-03-12T20:09:52.277226Z","iopub.status.idle":"2022-03-12T20:09:53.649405Z","shell.execute_reply.started":"2022-03-12T20:09:52.277189Z","shell.execute_reply":"2022-03-12T20:09:53.648723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Picked a Small Batch of data to choose a best model for classification**","metadata":{}},{"cell_type":"code","source":"def get_part_data(df, num_reviews):\n    \n    num_pos_reviews = df[df[\"is_spoiler\"]== True].shape[0]  # Number of spoilers in the dataset\n    num_neg_reviews = df[df[\"is_spoiler\"]== False].shape[0] # Number of Non-spoilers in the dataset\n    \n    fraction_pos = num_reviews/num_pos_reviews  # fraction of spoiler reviews to be returned\n    fraction_neg = num_reviews/num_neg_reviews  # fraction of non-spoiler reviews to be returned \n\n    df_pos = df[df['is_spoiler'] == True].sample(frac = fraction_pos, random_state = 2)\n    df_neg = df[df['is_spoiler'] == False].sample(frac = fraction_neg, random_state = 2)\n\n    df_re = pd.concat([df_pos, df_neg])  # join the True and False dataset\n    df_re = df_re.reset_index(drop=True)  # mix the index values\n    df_re.loc[(df_re['is_spoiler'] == True) ,'is_spoiler'] =1\n    df_re.loc[(df_re['is_spoiler'] == False) ,'is_spoiler'] =0\n    return df_re[[\"movie_id\",\"review_text\",\"review_summary\",\"is_spoiler\"]]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:53.650644Z","iopub.execute_input":"2022-03-12T20:09:53.651095Z","iopub.status.idle":"2022-03-12T20:09:53.659391Z","shell.execute_reply.started":"2022-03-12T20:09:53.651046Z","shell.execute_reply":"2022-03-12T20:09:53.658732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = get_part_data(df_reviews, 110000)\nd[\"is_spoiler\"] = d[\"is_spoiler\"].astype('int')\n\ndf_r = d[[\"review_text\",\"is_spoiler\"]]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:53.662703Z","iopub.execute_input":"2022-03-12T20:09:53.662913Z","iopub.status.idle":"2022-03-12T20:09:54.617261Z","shell.execute_reply.started":"2022-03-12T20:09:53.662883Z","shell.execute_reply":"2022-03-12T20:09:54.616533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:54.618709Z","iopub.execute_input":"2022-03-12T20:09:54.618961Z","iopub.status.idle":"2022-03-12T20:09:54.628779Z","shell.execute_reply.started":"2022-03-12T20:09:54.618928Z","shell.execute_reply":"2022-03-12T20:09:54.628118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pre-process the reviews by removing the stop words, tokenizing and lemmitization**","metadata":{}},{"cell_type":"code","source":"import spacy\nimport en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:09:54.630258Z","iopub.execute_input":"2022-03-12T20:09:54.630725Z","iopub.status.idle":"2022-03-12T20:10:03.755824Z","shell.execute_reply.started":"2022-03-12T20:09:54.630689Z","shell.execute_reply":"2022-03-12T20:10:03.755006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lower the words in training data\nltexts = [[word.lower() for word in line.split()] for line in df_r[\"review_text\"]]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:03.757193Z","iopub.execute_input":"2022-03-12T20:10:03.757562Z","iopub.status.idle":"2022-03-12T20:10:18.063554Z","shell.execute_reply.started":"2022-03-12T20:10:03.757527Z","shell.execute_reply":"2022-03-12T20:10:18.062733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    text = ' '.join(text)\n    # tokenization and removing stop words\n    token = [t for t in nlp(text) if not t.is_stop] \n    # lemmatization\n    lemma = [lem.lemma_ for lem in token]\n    # removing the non-alphabetic words\n    return [' '.join(i for i in lemma if i.isalpha()) ]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.064838Z","iopub.execute_input":"2022-03-12T20:10:18.065106Z","iopub.status.idle":"2022-03-12T20:10:18.07175Z","shell.execute_reply.started":"2022-03-12T20:10:18.065071Z","shell.execute_reply":"2022-03-12T20:10:18.071039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ltexts[5])\n\n# sample preprocessing for one text in the data\nprint(preprocess(ltexts[5]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.072986Z","iopub.execute_input":"2022-03-12T20:10:18.074003Z","iopub.status.idle":"2022-03-12T20:10:18.143506Z","shell.execute_reply.started":"2022-03-12T20:10:18.073969Z","shell.execute_reply":"2022-03-12T20:10:18.14275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# proc_text = []\n# for i in range(len(ltexts)):\n#     proc_text.append(preprocess(ltexts[i]))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.144637Z","iopub.execute_input":"2022-03-12T20:10:18.144901Z","iopub.status.idle":"2022-03-12T20:10:18.148624Z","shell.execute_reply.started":"2022-03-12T20:10:18.14484Z","shell.execute_reply":"2022-03-12T20:10:18.147816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#proc_text[5]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.150164Z","iopub.execute_input":"2022-03-12T20:10:18.150679Z","iopub.status.idle":"2022-03-12T20:10:18.157297Z","shell.execute_reply.started":"2022-03-12T20:10:18.150619Z","shell.execute_reply":"2022-03-12T20:10:18.15664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Base line Dummy classifier**","metadata":{}},{"cell_type":"code","source":"### Base line Dummy classifier\nX_train, X_test, y_train, y_test = train_test_split(df_r[\"review_text\"], df_r[\"is_spoiler\"],\n                                                     test_size=0.30, random_state=9)\n# Dummy classifier model\nclf = DummyClassifier(strategy='stratified')\nclf.fit(X_train, y_train)\n    \n# Predict the train by using dummmy classifier\ndummy_train_pred = clf.predict(X_train)\ndummy_test_pred  = clf.predict(X_test)\n    \nprint(classification_report(y_train, dummy_train_pred))\nprint(classification_report(y_test,dummy_test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.158849Z","iopub.execute_input":"2022-03-12T20:10:18.159101Z","iopub.status.idle":"2022-03-12T20:10:18.524897Z","shell.execute_reply.started":"2022-03-12T20:10:18.159068Z","shell.execute_reply":"2022-03-12T20:10:18.524173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Naive Bayes Classifier**","metadata":{}},{"cell_type":"code","source":"is_spoiler = [\"True\",\"False\"]\n\n### Naive Bayes model to predict the spoilers\ndef model_pipe(df):\n    X_train, X_test, y_train, y_test = train_test_split(df[\"review_text\"], df[\"is_spoiler\"],\n                                                        test_size=0.30, random_state=9)\n    # pipline the process for text classification\n    pipe = Pipeline([(\"count_vectorizer\", CountVectorizer(stop_words=\"english\")),\n                     (\"tfidf_transformer\", TfidfTransformer()),\n                     (\"nb_classifier\", BernoulliNB())])\n\n    # fit the model to the train data\n    pipe.fit(X_train, y_train)\n\n    # predict the train values\n    train_pred = pipe.predict(X_train)\n\n    # predict the test\n    test_pred = pipe.predict(X_test)\n    \n    cm = confusion_matrix(y_test,test_pred)\n    fig,ax = plt.subplots(figsize=(10, 10))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=is_spoiler)\n    disp.plot(ax=ax)\n    plt.show()\n\n    print(classification_report(y_train, train_pred))\n    print(classification_report(y_test, test_pred))\n    print(\"AUC Score\")\n    print(roc_auc_score(y_test, test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.526121Z","iopub.execute_input":"2022-03-12T20:10:18.526351Z","iopub.status.idle":"2022-03-12T20:10:18.558968Z","shell.execute_reply.started":"2022-03-12T20:10:18.526318Z","shell.execute_reply":"2022-03-12T20:10:18.558142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pipe(df_r)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:10:18.560555Z","iopub.execute_input":"2022-03-12T20:10:18.561075Z","iopub.status.idle":"2022-03-12T20:11:50.85732Z","shell.execute_reply.started":"2022-03-12T20:10:18.561037Z","shell.execute_reply":"2022-03-12T20:11:50.855791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nGpipe = Pipeline([('count_vect',CountVectorizer(stop_words=\"english\")),\n                ('tfidf_transformer',TfidfTransformer()),\n                ('nbClassifier',BernoulliNB())])\n\n\nparameters = {\n    'count_vect__binary': [True, False],\n    'count_vect__ngram_range': [(1, 1), (1, 2)],\n    'nbClassifier__alpha': (1, 0.1),\n}\n\ngrid_search = GridSearchCV(Gpipe, parameters, cv=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:11:50.858894Z","iopub.execute_input":"2022-03-12T20:11:50.859145Z","iopub.status.idle":"2022-03-12T20:11:50.865113Z","shell.execute_reply.started":"2022-03-12T20:11:50.859111Z","shell.execute_reply":"2022-03-12T20:11:50.864421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clf = grid_search.fit(X_train, y_train)\n#print(grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:11:50.86657Z","iopub.execute_input":"2022-03-12T20:11:50.867102Z","iopub.status.idle":"2022-03-12T20:11:50.877211Z","shell.execute_reply.started":"2022-03-12T20:11:50.867065Z","shell.execute_reply":"2022-03-12T20:11:50.876474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#naive_pred = grid_search(X_test)\n#print(classification_report(y_test,naive_pred))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:11:50.878694Z","iopub.execute_input":"2022-03-12T20:11:50.87929Z","iopub.status.idle":"2022-03-12T20:11:50.885537Z","shell.execute_reply.started":"2022-03-12T20:11:50.879252Z","shell.execute_reply":"2022-03-12T20:11:50.884715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **XG Boost Classifier**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:11:50.887021Z","iopub.execute_input":"2022-03-12T20:11:50.887584Z","iopub.status.idle":"2022-03-12T20:11:50.9565Z","shell.execute_reply.started":"2022-03-12T20:11:50.887549Z","shell.execute_reply":"2022-03-12T20:11:50.955869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xgmodel_pipe(df):\n    X_train, X_test, y_train, y_test = train_test_split(df[\"review_text\"], df[\"is_spoiler\"],\n                                                        test_size=0.30, random_state=9)\n    # pipline the process for text classification\n    pipe = Pipeline([(\"count_vectorizer\", CountVectorizer(stop_words = \"english\")),\n                     (\"tfidf_transformer\", TfidfTransformer()),\n                     (\"xg_classifier\", xgb.XGBClassifier(eta = 0.75, objective=\"binary:logitraw\"))])\n\n    # fit the model to the train data\n    pipe.fit(X_train, y_train)\n\n    # predict the train values\n    train_pred = pipe.predict(X_train)\n\n    # predict the test\n    test_pred = pipe.predict(X_test)\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test,test_pred)\n    fig,ax = plt.subplots(figsize=(10, 10))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=is_spoiler)\n    disp.plot(ax=ax)\n    plt.show()\n    \n     # Classification report\n    print(classification_report(y_train, train_pred))\n    print(classification_report(y_test, test_pred))\n    print(\"AUC Score\")\n    print(roc_auc_score(y_test, test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:11:50.957741Z","iopub.execute_input":"2022-03-12T20:11:50.957997Z","iopub.status.idle":"2022-03-12T20:11:50.968245Z","shell.execute_reply.started":"2022-03-12T20:11:50.957962Z","shell.execute_reply":"2022-03-12T20:11:50.967541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgmodel_pipe(df_r)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:11:50.969525Z","iopub.execute_input":"2022-03-12T20:11:50.969921Z","iopub.status.idle":"2022-03-12T20:22:40.367397Z","shell.execute_reply.started":"2022-03-12T20:11:50.969875Z","shell.execute_reply":"2022-03-12T20:22:40.366658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec used for word embedding technique","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom functools import reduce\nimport gensim\nfrom gensim.models import Word2Vec\nnlp = spacy.load('en_core_web_lg')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:22:40.368845Z","iopub.execute_input":"2022-03-12T20:22:40.369265Z","iopub.status.idle":"2022-03-12T20:22:45.175833Z","shell.execute_reply.started":"2022-03-12T20:22:40.369227Z","shell.execute_reply":"2022-03-12T20:22:45.1751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_e = get_part_data(df_reviews, 20000)\ndata_e[\"is_spoiler\"] = data_e[\"is_spoiler\"].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:24:35.893006Z","iopub.execute_input":"2022-03-12T20:24:35.893268Z","iopub.status.idle":"2022-03-12T20:24:36.278449Z","shell.execute_reply.started":"2022-03-12T20:24:35.89324Z","shell.execute_reply":"2022-03-12T20:24:36.277719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To identify the spolier in the review text. I thought, whether the word2vector using word embedding technique may find the cosine similarity between the **Review_text** and the **Plot summary** of the respective movie.\n\nTherefore, I implemented the Cosine similarity function to check the IMDB review text.","metadata":{}},{"cell_type":"code","source":"# Join the reviews with the plot summary of respective movie in df_details dataset\nreframed_data = data_e.merge(df_details, how='left', on='movie_id')\nreframed_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:24:37.280442Z","iopub.execute_input":"2022-03-12T20:24:37.280918Z","iopub.status.idle":"2022-03-12T20:24:37.3276Z","shell.execute_reply.started":"2022-03-12T20:24:37.280883Z","shell.execute_reply":"2022-03-12T20:24:37.326924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train and test spilt for word embedding technique\nx_train, x_test, Y_train, Y_test = train_test_split(reframed_data[[\"review_text\",\"plot_summary\"]],reframed_data[\"is_spoiler\"],\n                                                    test_size=0.30, random_state=9)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:24:37.764013Z","iopub.execute_input":"2022-03-12T20:24:37.76471Z","iopub.status.idle":"2022-03-12T20:24:37.791453Z","shell.execute_reply.started":"2022-03-12T20:24:37.764664Z","shell.execute_reply":"2022-03-12T20:24:37.790776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import reduce\ndef tn(sentence):\n    return reduce(lambda x,y: x+y, [nlp.vocab[w].vector for w in sentence.split()])\n\ndef transform1(row):\n        s1 = tn(row.review_text)\n        s2 = tn(row.plot_summary)\n        return np.stack([s1,s2])\n    \ndef transform(X):\n        return np.concatenate(\n            [transform1(row).reshape(1, -1) for row in X.itertuples()]\n        )\n\ndef cosine_sim(arr1, arr2): \n    ### to predict the spoiler by comparing the review text with orginal plot summary\n    return np.dot(arr1, arr2) / (np.linalg.norm(arr1) * np.linalg.norm(arr2))\n\n\ndef transform2(row):\n        s1 = tn(row.review_text)\n        s2 = tn(row.plot_summary)\n        return cosine_sim(s1,s2)\n    \n    \ndef similarity_predict(data, threshold):   \n    simPred = []\n    for row in data.itertuples():\n        sim = transform2(row)\n        if sim >=threshold:\n            simPred.append(int(1))\n        else:\n            simPred.append(int(0))\n    return simPred","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:24:38.184433Z","iopub.execute_input":"2022-03-12T20:24:38.184996Z","iopub.status.idle":"2022-03-12T20:24:38.19629Z","shell.execute_reply.started":"2022-03-12T20:24:38.18496Z","shell.execute_reply":"2022-03-12T20:24:38.195433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# By using the cosine similarity to identify which reviews having the spoiler\na = similarity_predict(x_train,0.9)\nb = similarity_predict(x_test,0.9)\n\nsim_train_pred = pd.DataFrame(a)\nsim_test_pred  = pd.DataFrame(b)\n\nprint(classification_report(sim_train_pred,Y_train))\nprint(classification_report(sim_test_pred,Y_test))\n\n# print(\"AUC Score\")\n# from sklearn.metrics import roc_auc_score\n# print(roc_auc_score(Y_test, sim_test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:24:38.656793Z","iopub.execute_input":"2022-03-12T20:24:38.657019Z","iopub.status.idle":"2022-03-12T20:25:49.640748Z","shell.execute_reply.started":"2022-03-12T20:24:38.656992Z","shell.execute_reply":"2022-03-12T20:25:49.639996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## USing MLP Regressor in word embedding for the classification of spoilers","metadata":{}},{"cell_type":"code","source":"# transform the trainng and development data\nprocessed_train = transform(x_train)\nprocessed_test  = transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:28:51.679593Z","iopub.execute_input":"2022-03-12T20:28:51.680332Z","iopub.status.idle":"2022-03-12T20:30:00.349001Z","shell.execute_reply.started":"2022-03-12T20:28:51.68029Z","shell.execute_reply":"2022-03-12T20:30:00.348181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# multilayer perceptron clssifier \nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier( max_iter=80,hidden_layer_sizes=(400,400,400)).fit(processed_train,Y_train) # y - gold_label of df_train\n\ntest_pred_processed = clf.predict(processed_test)\nprint(classification_report(test_pred_processed,Y_test)) ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:30:00.351689Z","iopub.execute_input":"2022-03-12T20:30:00.351976Z","iopub.status.idle":"2022-03-12T20:35:16.575159Z","shell.execute_reply.started":"2022-03-12T20:30:00.351922Z","shell.execute_reply":"2022-03-12T20:35:16.574392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(Y_test, test_pred_processed))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:35:16.579528Z","iopub.execute_input":"2022-03-12T20:35:16.579947Z","iopub.status.idle":"2022-03-12T20:35:16.593116Z","shell.execute_reply.started":"2022-03-12T20:35:16.579906Z","shell.execute_reply":"2022-03-12T20:35:16.592162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}